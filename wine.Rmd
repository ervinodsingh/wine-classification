---
title: "Wine classification in R"
author: "Giacomo Debidda"
date: "Saturday, December 27, 2014"
output: html_document
---

I have been practicing machine learning for the last few months. I read many tutorials like [these ones](https://www.youtube.com/user/mathematicalmonk/videos), watched many videos and [webinars hosted by RStudio](https://github.com/rstudio/webinars), and attended MOOCs like the amazing [Machine Learning from Andrew Ng on Coursera](https://www.coursera.org/course/ml). There is literally a ton of good (and free) stuff out there. Recently I came across a [really nice tutorial (pdf link)](http://www3.nd.edu/~mclark19/learn/ML.pdf) which shows how to train and compare several classifiers in R with the [caret package](http://topepo.github.io/caret/index.html).  
When you want to try out a particular model in R, you know that probably someone already developed an implementation for that and submitted a package to CRAN. The problem is that the syntax of every modeling function can be different. Caret solves this problem by providing an **interface** to train and test different models in a unique, consistent way (you still have to install those packages and load them every time you need them). Moreover, caret contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation
* model comparison

Here I draw inpiration from the aforementioned tutorial and I train a few classifiers to see which one shows the greatest performance over the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) UCI Machine Learning dataset.
There are many ways to measure the performance of a classifier: we can look at the confusion matrix (aka contingency table), we can plot the ROC curve, we can calculate the accuracy, etc. Here, we are interested in predicting if a given wine is good or bad, so this problem is a clear example of **binary classification**, and for comparing the classifiers we are going to use the **accuracy**. In this tutorial we are going to evaluate the following 8 classifiers:

1. Naive Bayes (NB)
2. Logistic regression (GLM)
3. K Nearest Neighbor (KNN)
4. Support Vector Machines (SVM)
5. Support Vector Machines with Radial Basis Function (SVM_RBF)
6. Neural Networks (NNet)
7. Neural Networks average (AvgNNet)
8. Random Forest (RF)

Every classifer can have some parameters to tune in caret, see [here](http://topepo.github.io/caret/modelList.html) to know which ones. In order to find the best set of parameters, caret tries all the combinations with a [grid search optimization algorithm](http://en.wikipedia.org/wiki/Hyperparameter_optimization). I know that there are several others optimization algorithms, but grid search is probably the most used, and it's the only option if you want to use caret.

### Load the required packages and enable parallel processing

```{r Load Packages, echo=FALSE}
library(corrplot)  # graphical display of the correlation matrix
library(caret)     # classification and regression training
library(klaR)      # naive bayes
library(nnet)      # neural networks (nnet and avNNet)
library(kernlab)   # support vector machines (svmLinear and svmRadial)
library(randomForest)  # random forest, also for recursive feature elimination
library(gridExtra) # save dataframes as images

library(doSNOW)    # parallel processing
registerDoSNOW(makeCluster(3, type = 'SOCK')) 
# Note: in caret, the functions train, rfe, sbf, bag and avNNet were given an additional argument in their respective control files called allowParallel that defaults to TRUE.

today <- as.character(Sys.Date())
```

### Load the data

As Hadley Wickham says, data scientists spend roughly 80% of their time dealing with messy data, and they often have to perform some data wrangling in order to prepare the data for the analysis. Hadley writes about these techniques in his paper [tidy data (pdf link)](http://www.jstatsoft.org/v59/i10/paper).  
Tidy data have three characteristics:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

The original wine data consist of 2 CSV files, one for white wines, one for red wines. The author of the original tutorial combined them and created additional variables: color (white or red), and good (indicating scores greater than or equal to 6). We are going to use the same approach.  
Note: it seems there are some issues when knitr has to download files with https, so I'm going to use http. See [here](http://stackoverflow.com/questions/25341285/error-when-knitr-has-to-download-a-zip-file) and [here](http://stackoverflow.com/questions/19890633/r-produces-unsupported-url-scheme-error-when-getting-data-from-https-sites) for more details about these issues.

```{r Get the datasets from UCI machine learning, echo=FALSE}
# Note: use http, it does not work with https in knitr (it does work from the R console)
red <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv', header = TRUE, sep = ';')
white <- read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv', header = TRUE, sep = ';')
```

```{r merge the 2 datasets, echo=FALSE}
red[, 'color'] <- 'red'
white[, 'color'] <- 'white'
df <- rbind(red, white)
df$color <- as.factor(df$color)
```

### Change the type of problem: from regression to binary classification

We are interested in predicting the quality of the wine. In the original dataset, quality is a **quantitative variable**. Let's say that if a wine has `{r quality > 6}` we can classify that wine as `{r good}`. Otherwise, that wine is `{r bad}`. So, for our binary classification problem we are going to define a **factor variable**, and we are going to use it as response variable (a.k.a. output, outcome, target, etc). Since there is no need to keep the original quality variable (quantitative), we are going to redefine it as factor variable.
 
```{r change to binary classification, echo=FALSE}
good_ones <- df$quality >= 6
bad_ones <- df$quality < 6
df[good_ones, 'quality'] <- 'good'
df[bad_ones, 'quality'] <- 'bad'  
df$quality <- as.factor(df$quality)
```

```{r dummy variables, echo=FALSE}
dummies <- dummyVars(quality ~ ., data = df)
df_dummied <- data.frame(predict(dummies, newdata = df))
df_dummied[, 'quality'] <- df$quality
```

### Data splitting

Caret has its own partitioning function to split the data into training set and test set. Here we are going to assign 70% of the observations to the training set, and the remaining 30% to the test set. Since the dataset is not particurarly big, we are not going to use a validation set to tune the model parameters. However, it is advisable to split the data into 3 sets whenever you can, otherwise the apparent performance of a model, as evaluated on the training set, will be overly optimistic. The workflow should be: train each model using the training set, choose the best model using the validation set, generate the final results using the test set. Read [here](http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set) and [here](http://stats.stackexchange.com/questions/79905/cross-validation-including-training-validation-and-testing-why-do-we-need-thr?lq=1) a couple of great answers on Cross Validated that explain the reasons why 3 sets are needed.

```{r split the data, echo=FALSE}
# set the seed for reproducibility
set.seed(1234) 
trainIndices <- createDataPartition(df_dummied$quality, p = 0.7, list = FALSE)
train <- df_dummied[trainIndices, ]
test <- df_dummied[-trainIndices, ]
```

### Feature selection

This dataset is composed by 6497 observations (rows) and 15 variables (columns). When we build a model, we want to include only the relevant features. We don't want to include highly correlated features in our models.  
There are several ways to do a feature selection. For example:

* take a look at the correlation matrix
* perform a backwards selection (a.k.a. recursive feature elimination)
* perform a forward selection

**Option 1: feature selection by looking at the correlation matrix**
Since we want to avoid collinearity in the data, we can take a look at correlation matrix. We can't include `{r quality}` because it is the response variable. And we cannot include `{r color}` because is a factor variable.

```{r correlation matrix, echo=FALSE}
numericColumns <- !colnames(train) %in% c('quality', 'color.red', 'color.white')
correlationMatrix <- cor(train[, numericColumns])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff = 0.6)
colnames(correlationMatrix)[highlyCorrelated]

png(paste0(today, '-', 'correlation-matrix.png'))
corrplot(correlationMatrix, method = 'number', tl.cex = 0.5)
dev.off()
```

If we set a threshold (e.g. 0.6), we can see which features are highly correlated. By looking at the correlation matrix we can conclude that `{r total.sulfur.dioxide}` and `{r density}` are linearly correlated with other variables, and therefore we are not going not include them in any classifier.

**Option 2: feature selection by recursive feature elimination (aka backward elimination)**
We cannot include in our classifiers the variable `{r quality}` (response variable).
With this approach we can see how the accuracy of the classifier changes by including more features. According to this approach we should exclude `{r density}` but include `{r total.sulfur.dioxide}`.
Note: this approach is really slow (~3 minutes with parallel processing enabled on my computer, using 3 cores). See the documentation on the [caret website](http://topepo.github.io/caret/featureselection.html).
Read also [here](http://www.brainvoyager.com/bvqx/doc/UsersGuide/MVPA/RecursiveFeatureElimination.html) and [here](http://stats.stackexchange.com/questions/103810/find-variables-selected-for-each-subset-using-caret-feature-selection) for further details.

```{r RFE with Random Forest, echo=FALSE, cache=TRUE}
fitControl_rfe <- rfeControl(functions = rfFuncs, method = 'cv', number = 5) # 5-fold CV
fit_rfe <- rfe(quality ~., data = train,
               sizes = c(1:10),  # subset sizes to test (ahem... not sure how it works)
               rfeControl = fitControl_rfe)
features <- predictors(fit_rfe) # same command as fit_rfe$optVariables
max(fit_rfe$results$Accuracy)

png(paste0(today, '-', 'recursive-feature-elimination.png'))
plot(fit_rfe, type = c('g', 'o'), main = 'Recursive Feature Elimination')
dev.off()
```

### Exploratory Data Analysis

Let's see how the variables are distributed.

```{r exploratory data analysis, echo=FALSE}
# Normalize the quantitative variables to be within the [0,1] range
train_normalized <- preProcess(train[, numericColumns], method = 'range')
train_plot <- predict(train_normalized, train[, numericColumns])

# Let'€™s take an initial peek at how the predictors separate on the target
png(paste0(today, '-', 'feature-plot.png'))
featurePlot(train_plot, train$quality, 'box')
dev.off()
```

Note: using the **preProcess** function when some variables are factors is a bit tricky. See [how to preprocess factors](http://stackoverflow.com/questions/14023423/how-to-preprocess-features-when-some-of-them-are-factors).

For the training set, it looks like `{r alcohol}`, `{r volatile.acidity}` and `{r chlorides}` separate most with regard to good classification. While this might give us some hints, the figure does not provide us any suggestions about interaction effects which may occur between the variables.

### Resampling

The function **trainControl** in the caret package is a very convenient way to set the resampling scheme we need. Among the resampling methods it accepts as argument, we can choose bootstrap, cross validation, and repeated cross validation. Of course there are some [differences between bootstrap and CV to estimate the prediction error](http://stats.stackexchange.com/questions/18348/differences-between-cross-validation-and-bootstrapping-to-estimate-the-predictio). And for the repeated K-fold cross validation, we have to decide the number of folds (K) and the number of separate repetions. [Here](http://stats.stackexchange.com/questions/82546/how-many-times-should-we-repeat-a-k-fold-cv) there are some guidelines about how many repetitions to perform. However, since there are so many models to train, a repeated K-fold CV can be very slow.

```{r cross validation}
# fitControl <- trainControl(method = 'repeatedcv', number = 5, repeats = 3)  # very slow
fitControl <- trainControl(method = 'cv', number = 5)
```

Ok, now that we have a standard way to evaluate all our classifiers, let's start with the simplest models.

### Naive Bayes classifier (NB)

```{r Naive Bayes}
# Here the Naive Bayes classifier works best without range
fit_nb <- train(x = train[, features], y = train$quality,
                method ='nb',
                trControl = fitControl)
predict_nb <- predict(fit_nb, newdata = test[, features])
confMat_nb <- confusionMatrix(predict_nb, test$quality, positive = 'good')
importance_nb <- varImp(fit_nb, scale = TRUE)

png(paste0(today, '-', 'importance-nb.png'))
plot(importance_nb, main = 'Feature importance for Naive Bayes')
dev.off()
```

More resources for Naive Bayes classifiers in R [here](http://joshwalters.com/2012/11/27/naive-bayes-classification-in-r.html).

### K Nearest Neighbors (KNN)

```{r KNN}
# tunable parameter: k = number of neighbours (set k as an odd number to avoid ties)
fit_knn <- train(x = train[, features], y = train$quality,
                 method = 'knn',
                 preProcess = 'range', 
                 trControl = fitControl, 
                 tuneGrid = expand.grid(.k = c(3, 5, 7, 9, 11, 15, 21, 25, 31, 41, 51, 75, 101)))  
predict_knn <- predict(fit_knn, newdata = test[, features])
confMat_knn <- confusionMatrix(predict_knn, test$quality, positive = 'good')
importance_knn <- varImp(fit_knn, scale = TRUE)

png(paste0(today, '-', 'importance-knn.png'))
plot(importance_knn, main = 'Feature importance for K Nearest Neighbors')
dev.off()
```

### Logistic Regression (GLM)

```{r Logistic Regression}
# no tunable parameters
fit_glm <- train(x = train[, features], y = train$quality,
                 method = 'glm',
                 preProcess = 'range', # it shows the same result with or without 'range'
                 trControl = fitControl) 
predict_glm <- predict(fit_glm, newdata = test[, features])
confMat_glm <- confusionMatrix(predict_glm, test$quality, positive = 'good')
importance_glm <- varImp(fit_glm, scale = TRUE)

png(paste0(today, '-', 'importance-glm.png'))
plot(importance_glm, main = 'Feature importance for Logistic Regression')
dev.off()
```

### Support Vector Machines with linear kernel (svmLinear)

```{r SVM Linear, cache=TRUE}
# tunable parameter: C = regularization parameter which sets the decision boundary
# In svmLinear we cannot tune sigma
# Note: the training is very slow
fit_svm <- train(x = train[, features], y = train$quality,
                 method = 'svmLinear',
                 preProcess = 'range',
                 trControl = fitControl,
                 tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 1, 10, 100, 1000)))
predict_svm <- predict(fit_svm, newdata = test[, features])
confMat_svm <- confusionMatrix(predict_svm, test$quality, positive = 'good')
importance_svm <- varImp(fit_svm, scale = TRUE)

png(paste0(today, '-', 'importance-svm.png'))
plot(importance_svm, main = 'Feature importance for SVM-Linear')
dev.off()
```

The accuracy isn't great. Maybe we could try with a different kernel function. A very popular choice is the Radial Basis kernel function (RBF).

### Support Vector Machines with Radial Basis Function (svmRBF)

```{r SVM RBS, cache=TRUE}
# tunable parameter:
# C = regularization parameter which sets the decision boundary;
# sigma = kernel.
# Note: the training is very slow                                         
fit_svmRBF <- train(x = train[, features], y = train$quality,
                    method = 'svmRadial',
                    preProcess = 'range',
                    trControl = fitControl,
                    tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
                                           .sigma = c(0.001, 0.01, 0.1)))
predict_svmRBF <- predict(fit_svmRBF, newdata = test[, features])
confMat_svmRBF <- confusionMatrix(predict_svmRBF, test$quality, positive = 'good')
importance_svmRBF <- varImp(fit_svmRBF, scale = TRUE)

png(paste0(today, '-', 'importance-svmRBF.png'))
plot(importance_svmRBF, main = 'Feature importance for SVM-RBF')
dev.off()
```

The SVM with linear kernel does not perform particularly well, neither does the SVM-RBF classifier.

### Neural Networks (NNet)

```{r Neural Networks, cache=TRUE}
# tunable parameters:
# size = number of units in the hidden layer;
# decay = weight decay of the neural network (regularization parameter)
# Note: the training is very slow
fit_nnet <- train(x = train[, features], y = train$quality,
                  method = 'nnet',
                  preProcess = 'range',
                  trControl = fitControl,
                  tuneGrid = expand.grid(.size = c(1, 5, 10),
                                         .decay = c(0, 0.001, 0.1)),
                  trace = FALSE,  # switch for tracing optimization. Default TRUE
                  maxit = 1000)   # maximum number of iterations. Default 100
predict_nnet <- predict(fit_nnet, newdata = test[, features], type = 'raw')
confMat_nnet <- confusionMatrix(predict_nnet, test$quality, positive = 'good')
importance_nnet <- varImp(fit_nnet, scale = TRUE)

png(paste0(today, '-', 'importance-nnet.png'))
plot(importance_nnet, main = 'Feature importance for NNet')
dev.off()
```

The Neural Networks classifier performs slightly better than the KNN classifier, but nothing spectacular. And it is really slow to train.

### Neural Networks average (AvNNet)

The avNNet method in the caret package train several neural networks classifiers, aggregate them and average them. It is an ensemble method.

```{r AvNNet, cache=TRUE}
# tunable parameters:
# size = number of units in the hidden layer;
# decay = weight decay of the neural network (regularization parameter);
# bag = a logical for bagging for each repeat.
# Note: the training is very slow
fit_avNnet <- train(x = train[, features], y = train$quality,
                    method = 'avNNet',
                    preProcess = 'range',
                    trControl = fitControl,
                    tuneGrid = expand.grid(.size = c(1, 5, 10),
                                           .decay = c(0, 0.001, 0.1),
                                           .bag = FALSE),
                    trace = FALSE,
                    maxit = 1000)
predict_avNnet <- predict(fit_avNnet, newdata = test[, features])
confMat_avNnet <- confusionMatrix(predict_avNnet, test$quality, positive = 'good')
importance_avNnet <- varImp(fit_avNnet, scale = TRUE)

png(paste0(today, '-', 'importance-avNnet.png'))
plot(importance_avNnet, main = 'Feature importance for avNnet')
dev.off()
```

The Model Averaged Neural Networks classifier performs slightly better than the single Neural Networks.

### Random Forest (RF)

```{r Random Forest, cache=TRUE}
# tunable parameter: mtry (number of variables randomly sampled as candidates at each split)
fit_rf <- train(x = train[, features], y = train$quality,
                 method = 'rf',
                 # preProcess = 'range', # it seems slightly better without 'range'
                 trControl = fitControl,
                tuneGrid = expand.grid(.mtry = c(2:6)),
                n.tree = 1000) 
predict_rf <- predict(fit_rf, newdata = test[, features])
confMat_rf <- confusionMatrix(predict_rf, test$quality, positive = 'good')
importance_rf <- varImp(fit_rf, scale = TRUE)

png(paste0(today, '-', 'importance-rf.png'))
plot(importance_rf, main = 'Feature importance for Random Forest')
dev.off()
```

The Random Forest classifier performs quite better than the Model Averaged Neural Networks, and it is significantly faster to train. A random forest classifier does not suffer of irrelevant and/or correlated predictors, and since it is an ensemble method, it contains the combined information of many models, so it is lesser prone to overfitting.  
Note: As Jeremy Howard says [here](https://www.youtube.com/watch?v=kwt6XEh7U3g#t=47m22s), you should **never prune trees** in a Random Forests classifier.

### Compare the models

```{r compare models}
models <- resamples(list(NB = fit_nb, KNN = fit_knn, GLM = fit_glm,
                         SVM = fit_svm,
                         SVM_RBF = fit_svmRBF,
                         NNet = fit_nnet, AvgNNet = fit_avNnet,
                         RF = fit_rf))
png(paste0(today, '-', 'compare-models.png'))
dotplot(models)
dev.off()

results <- summary(models)
png(paste0(today, '-', 'models-accuracy.png'), width = 480, height = 240)
grid.table(results$statistics$Accuracy)
dev.off()
png(paste0(today, '-', 'models-kappa.png'), width = 480, height = 240)
grid.table(results$statistics$Kappa)
dev.off()
```

### A note on reproducibility

I set a seed in the data splitting phase, so evry time the code runs, we will obtain the same training set and test set over and over again. However, I didn't set a seed in the classifiers, so the resampling phase does **not** provide the same results every time a model is trained. For a fully reproducible example we could [set a seed in the trainControl caret function](http://stackoverflow.com/questions/13403427/fully-reproducible-parallel-models-using-caret).
